{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging;\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger().setLevel(level=logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resample_3600s_unstack_double_24_lstm_timeseries_classifier',\n",
       " 'resample_3600s_unstack_24_lstm_timeseries_classifier',\n",
       " 'resample_600s_unstack_144_lstm_timeseries_classifier',\n",
       " 'resample_600s_normalize_dfs_1d_xgb_classifier',\n",
       " 'resample_600s_unstack_dfs_1d_xgb_classifier',\n",
       " 'resample_600s_unstack_double_144_lstm_timeseries_classifier',\n",
       " 'resample_600s_unstack_normalize_dfs_1d_xgb_classifier']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from greenguard import get_pipelines\n",
    "\n",
    "get_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenguard.demo import load_demo\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from greenguard.pipeline import GreenGuardPipeline\n",
    "\n",
    "\n",
    "target_times, readings = load_demo()\n",
    "\n",
    "template = 'resample_600s_unstack_double_144_lstm_timeseries_classifier'\n",
    "window_size = 84\n",
    "rule = '4h'\n",
    "\n",
    "init_params = [{\n",
    "    'pandas.DataFrame.resample#1': {\n",
    "        'rule': rule,\n",
    "    },\n",
    "    'mlprimitives.custom.timeseries_preprocessing.cutoff_window_sequences#1': {\n",
    "        'window_size': window_size,\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_template(template, metric, target_times, readings, tuning_iterations,\n",
    "                   init_params=None, cost=False, test_size=0.25, cv_splits=3, random_state=0):\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        train, test = train_test_split(target_times, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        pipeline = GreenGuardPipeline(template, metric, cost=cost, cv_splits=cv_splits, init_params=init_params)\n",
    "\n",
    "        #Computing the default test score\n",
    "        pipeline.fit(train, readings)\n",
    "        predictions = pipeline.predict(test, readings)\n",
    "\n",
    "        scores['default_test'] = f1_score(test['target'], predictions)\n",
    "\n",
    "        #Computing de default cross validation score\n",
    "        gc.collect()\n",
    "        session = pipeline.tune(train, readings)\n",
    "        session.run(1)\n",
    "\n",
    "        scores['default_cv'] = pipeline.cv_score\n",
    "\n",
    "        #Computing the cross validation score with tuned hyperparameters\n",
    "        session.run(tuning_iterations)    \n",
    "        pipeline.get_hyperparameters()\n",
    "\n",
    "        scores['tuned_cv'] = pipeline.cv_score\n",
    "\n",
    "        #Computing the test score with tuned hyperparameters\n",
    "        pipeline.fit(train, readings)\n",
    "        predictions = pipeline.predict(test, readings)\n",
    "\n",
    "        scores['tuned_test']= f1_score(test['target'], predictions)\n",
    "    \n",
    "    except:\n",
    "        return scores\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mlblocks.mlpipeline:Exception caught fitting MLBlock keras.Sequential.DoubleLSTMTimeSeriesClassifier#1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/usuario/.virtualenvs/GreenGuard/lib/python3.6/site-packages/mlblocks/mlpipeline.py\", line 549, in _fit_block\n",
      "    block.fit(**fit_args)\n",
      "  File \"/home/usuario/.virtualenvs/GreenGuard/lib/python3.6/site-packages/mlblocks/mlblock.py\", line 302, in fit\n",
      "    getattr(self.instance, self.fit_method)(**fit_kwargs)\n",
      "  File \"/home/usuario/.virtualenvs/GreenGuard/lib/python3.6/site-packages/mlprimitives/adapters/keras.py\", line 111, in fit\n",
      "    shuffle=self.shuffle)\n",
      "  File \"/home/usuario/.virtualenvs/GreenGuard/lib/python3.6/site-packages/keras/engine/training.py\", line 1154, in fit\n",
      "    batch_size=batch_size)\n",
      "  File \"/home/usuario/.virtualenvs/GreenGuard/lib/python3.6/site-packages/keras/engine/training.py\", line 579, in _standardize_user_data\n",
      "    exception_prefix='input')\n",
      "  File \"/home/usuario/.virtualenvs/GreenGuard/lib/python3.6/site-packages/keras/engine/training_utils.py\", line 135, in standardize_input_data\n",
      "    'with shape ' + str(data_shape))\n",
      "ValueError: Error when checking input: expected lstm_5_input to have 3 dimensions, but got array with shape (264, 1)\n"
     ]
    }
   ],
   "source": [
    "scores = score_template(\n",
    "    template=template,\n",
    "    metric=f1_score, \n",
    "    target_times=target_times, \n",
    "    readings=readings, \n",
    "    tuning_iterations=50,    \n",
    "    init_params=init_params, \n",
    "    cost=False, \n",
    "    test_size=0.25, \n",
    "    cv_splits=3,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Result\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
