{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging;\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger().setLevel(level=logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['resample_3600s_unstack_double_24_lstm_timeseries_classifier',\n",
       " 'resample_3600s_unstack_24_lstm_timeseries_classifier',\n",
       " 'resample_600s_unstack_144_lstm_timeseries_classifier',\n",
       " 'resample_600s_normalize_dfs_1d_xgb_classifier',\n",
       " 'resample_600s_unstack_dfs_1d_xgb_classifier',\n",
       " 'resample_600s_unstack_double_144_lstm_timeseries_classifier',\n",
       " 'resample_600s_unstack_normalize_dfs_1d_xgb_classifier']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from greenguard import get_pipelines\n",
    "\n",
    "get_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_template(template, metric, target_times, readings, tuning_iterations,\n",
    "                   init_params=None, cost=False, test_size=0.25, cv_splits=3, random_state=0):\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        train, test = train_test_split(target_times, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        pipeline = GreenGuardPipeline(template, metric, cost=cost, cv_splits=cv_splits, init_params=init_params)\n",
    "\n",
    "        #Computing the default test score\n",
    "        pipeline.fit(train, readings)\n",
    "        predictions = pipeline.predict(test, readings)\n",
    "\n",
    "        scores['default_test'] = f1_score(test['target'], predictions)\n",
    "\n",
    "        #Computing de default cross validation score\n",
    "        gc.collect()\n",
    "        session = pipeline.tune(train, readings)\n",
    "        session.run(1)\n",
    "\n",
    "        scores['default_cv'] = pipeline.cv_score\n",
    "\n",
    "        #Computing the cross validation score with tuned hyperparameters\n",
    "        session.run(tuning_iterations)    \n",
    "        pipeline.get_hyperparameters()\n",
    "\n",
    "        scores['tuned_cv'] = pipeline.cv_score\n",
    "\n",
    "        #Computing the test score with tuned hyperparameters\n",
    "        pipeline.fit(train, readings)\n",
    "        predictions = pipeline.predict(test, readings)\n",
    "\n",
    "        scores['tuned_test']= f1_score(test['target'], predictions)\n",
    "    \n",
    "    except:\n",
    "        return scores\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the init_params of lstm\n",
    "import pandas as pd\n",
    "\n",
    "def build_lstm_init_params(rule, window_size):\n",
    "    window_size = int(pd.to_timedelta(window_size) / pd.to_timedelta(rule))\n",
    "    return [{\n",
    "        'pandas.DataFrame.resample#1': {\n",
    "            'rule': rule,\n",
    "        },\n",
    "        'featuretools.dfs.json#1': {\n",
    "            'window_size': window_size,\n",
    "        }\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the init_params of the dfs\n",
    "def build_dfs_init_params(rule, window_size):\n",
    "    return [{\n",
    "        'pandas.DataFrame.resample#1': {\n",
    "            'rule': rule,\n",
    "        },\n",
    "        'mlprimitives.custom.timeseries_preprocessing.cutoff_window_sequences#1': {\n",
    "            'training_window': window_size,\n",
    "        }\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluates the score of a pipeline with diferents window_size and rule    \n",
    "def evaluate_template(template, window_rule_size, metric, tuning_iterations, cost=False,\n",
    "                     test_size=0.25, cv_splits=3, random_state=0):\n",
    "    scores_list = []\n",
    "    \n",
    "    INIT_PARAMS_BUILDERS = {\n",
    "    'resample_600s_normalize_dfs_1d_xgb_classifier': build_dfs_init_params,\n",
    "    'resample_600s_unstack_double_144_lstm_timeseries_classifier': build_lstm_init_params,\n",
    "    }\n",
    "    \n",
    "    target_times, readings = load_demo()\n",
    "    \n",
    "    for x in window_rule_size:\n",
    "        window_size = x[0]\n",
    "        rule = x[1]\n",
    "        \n",
    "        init_params_builder = INIT_PARAMS_BUILDERS[template]\n",
    "        init_params = init_params_builder(rule, window_size)\n",
    "    \n",
    "        scores = score_template(\n",
    "            template=template,\n",
    "            metric=f1_score, \n",
    "            target_times=target_times, \n",
    "            readings=readings, \n",
    "            tuning_iterations=50,    \n",
    "            init_params=init_params, \n",
    "            cost=False, \n",
    "            test_size=0.25, \n",
    "            cv_splits=3,\n",
    "            random_state=0)\n",
    "        \n",
    "        scores['template'] = template\n",
    "        scores['window_size'] = window_size\n",
    "        scores['rule'] = rule\n",
    "        scores_list.append(scores)\n",
    "   \n",
    "    return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenguard.demo import load_demo\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from greenguard.pipeline import GreenGuardPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_rule_size = {\n",
    "    ('30d','12h'),\n",
    "    ('30d','1d')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 99 features\n",
      "Elapsed: 00:45 | Progress: 100%|██████████\n",
      "Elapsed: 00:16 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:31 | Progress: 100%|██████████\n",
      "Elapsed: 00:16 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:33 | Progress: 100%|██████████\n",
      "Elapsed: 00:15 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:30 | Progress: 100%|██████████\n",
      "Elapsed: 00:16 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:50 | Progress: 100%|██████████\n",
      "Elapsed: 00:17 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:49 | Progress: 100%|██████████\n",
      "Elapsed: 00:16 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:33 | Progress: 100%|██████████\n",
      "Elapsed: 00:16 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:32 | Progress: 100%|██████████\n",
      "Elapsed: 00:16 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:34 | Progress: 100%|██████████\n",
      "Elapsed: 00:15 | Progress: 100%|██████████\n",
      "Built 99 features\n",
      "Elapsed: 00:46 | Progress: 100%|██████████\n",
      "Elapsed: 00:15 | Progress: 100%|██████████\n"
     ]
    }
   ],
   "source": [
    "#We evaluate the score of the pipeline with the following parameters\n",
    "template = 'resample_600s_normalize_dfs_1d_xgb_classifier'\n",
    "\n",
    "scores_list = evaluate_template(\n",
    "            template=template, \n",
    "            window_rule_size=window_rule_size,\n",
    "            metric=f1_score, \n",
    "            tuning_iterations=50, \n",
    "            cost=False,\n",
    "            test_size=0.25, \n",
    "            cv_splits=3, \n",
    "            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'default_test': 0.6415094339622641,\n",
       "  'default_cv': 0.6759173604687018,\n",
       "  'tuned_cv': 0.6897163120567377,\n",
       "  'tuned_test': 0.6792452830188679,\n",
       "  'template': 'resample_600s_normalize_dfs_1d_xgb_classifier',\n",
       "  'window_size': '30d',\n",
       "  'rule': '12h'},\n",
       " {'default_test': 0.7058823529411765,\n",
       "  'default_cv': 0.73645390070922,\n",
       "  'tuned_cv': 0.7512338268640789,\n",
       "  'tuned_test': 0.7450980392156863,\n",
       "  'template': 'resample_600s_normalize_dfs_1d_xgb_classifier',\n",
       "  'window_size': '30d',\n",
       "  'rule': '1d'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result\n",
    "scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
